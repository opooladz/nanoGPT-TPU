default: &default # Lets you use the same config file with different defaults
  # Default config values for nanoGPT
  # -----------------------------------------------------------------------------
  # I/O
  out_dir: 'out'
  eval_interval: 2000
  log_interval: 1
  eval_iters: 20
  eval_only: False # if True, script exits right after the first eval
  always_save_checkpoint: True # if True, always save a checkpoint after each eval
  init_from: 'scratch' # 'scratch' or 'resume' or 'gpt2*'
  # -----------------------------------------------------------------------------
  # wandb
  wandb: False
  wandb_project: 'owt'
  wandb_run_name: 'gpt2'
  # -----------------------------------------------------------------------------
  # data
  # Dataset based on OpenWebText but refactored to be more manageable
  dataset: 'openwebtext' 
  gradient_accumulation_steps: 40 # originally 5 * 8 but yaml can't handle it needs to an integer
  batch_size: 12 # if gradient_accumulation_steps > 1, this is the micro-batch size
  block_size: 1024
  # -----------------------------------------------------------------------------
  # model
  n_layer: 12
  n_head: 12
  n_embd: 764
  dropout: 0.0 # for pretraining 0 is good, for finetuning try 0.1+
  bias: False # do we use bias inside LayerNorm and Linear layers?
  # -----------------------------------------------------------------------------
  # adamw optimizer
  # We use the !!!float yaml tag to force the yaml parser to read the value as a float
  learning_rate: !!float 6e-4 # max learning rate
  max_iters: 60000 # total number of training iterations
  weight_decay: !!float 1e-1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0 # clip gradients at this value, or disable if:= 0.0
  # -----------------------------------------------------------------------------
  # learning rate decay settings
  decay_lr: True # whether to decay the learning rate
  warmup_iters: 2000 # how many steps to warm up for
  lr_decay_iters: 60000 # should be ~= max_iters per Chinchilla
  min_lr: !!float 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla
  # -----------------------------------------------------------------------------
  # DDP settings
  backend: 'nccl' # 'nccl', 'gloo', etc.
  # -----------------------------------------------------------------------------
  # system
  device: 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks
  dtype: 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
  compile: True # use PyTorch 2.0 to compile the model to be faster
  # -----------------------------------------------------------------------------

# Cpu config
cpu: &cpu # Lets you use the same config file with different defaults
  <<: *default # use the same config as default
  eval_iters: 20
  # My cpu config for nanoGPT
  # Default block size is to large for cpu training so we need to reduce it
  block_size: 128 # Adjust this if you are running out of memory or want to increase block size
  n_layer: 4
  n_head: 4
  n_embd: 128
  max_iters: 2000
  lr_decay_iters: 2000
  backend: 'gloo' # Backend for CPU training
  device: 'cpu' # Change to 'cpu' if you don't have a GPU
  compile: False